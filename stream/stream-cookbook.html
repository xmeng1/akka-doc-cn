<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
<title>Streams Cookbook &bull; Akka Documentation 中文</title>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="description" content="akka-docs-cn"/>
<link rel="canonical" href="http://doc.akka.io/docs/akka/current/stream/stream-cookbook.html"/>
<script type="text/javascript" src="../lib/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../lib/foundation/dist/js/foundation.min.js"></script>
<link rel="stylesheet" type="text/css" href="../lib/normalize.css/normalize.css"/>
<link rel="stylesheet" type="text/css" href="../lib/foundation/dist/css/foundation.min.css"/>
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/docsearch.js/2/docsearch.min.css" />
<link rel="stylesheet" type="text/css" href="../css/icons.css"/>
<link rel="stylesheet" type="text/css" href="../css/page.css"/>
<link rel="shortcut icon" href="../images/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="../images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../images/favicon-16x16.png">
<link rel="manifest" href="../images/manifest.json">
<meta name="msapplication-TileImage" content="../images/mstile-150x150.png">
<meta name="msapplication-TileColor" content="#15a9ce">
<meta name="theme-color" content="#15a9ce">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- OneTrust Cookies Consent Notice (Production Standard, akka.io, en-GB) start -->
<script src="https://optanon.blob.core.windows.net/consent/159bb13d-6748-4399-806e-ac28db879785.js" type="text/javascript" charset="UTF-8"></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice (Production Standard, akka.io, en-GB) end -->
<!--Google Analytics-->
<script type="text/plain" class="optanon-category-2">
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-21117439-1']);
_gaq.push(['_setDomainName', 'akka.io']);
_gaq.push(['_trackPageview']);
(function() {
var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})()
</script>
<script type="text/plain" class="optanon-category-2">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-23127719-1', 'lightbend.com', {'allowLinker': true, 'name': 'tsTracker'});
ga('tsTracker.require', 'linker');
ga('tsTracker.linker:autoLink', ['lightbend.com','playframework.com','scala-lang.org','scaladays.org','spray.io','akka.io','scala-sbt.org','scala-ide.org']);
ga('tsTracker.send', 'pageview');
</script>
<!--Marketo-->
<script type="text/plain" class="optanon-category-3">
(function() {
var didInit = false;
function initMunchkin() {
if(didInit === false) {
didInit = true;
Munchkin.init('558-NCX-702', { 'asyncOnly': true, 'disableClickDelay': true });
}
}
var s = document.createElement('script');
s.type = 'text/javascript';
s.async = true;
s.src = '//munchkin.marketo.net/munchkin.js';
s.onreadystatechange = function() {
if (this.readyState == 'complete' || this.readyState == 'loaded') {
initMunchkin();
}
};
s.onload = initMunchkin;
document.getElementsByTagName('head')[0].appendChild(s);
})();
</script>
</head>

<body id="underlay" data-toggler="nav-open">

<header class="site-header hide-for-large">
<div class="sticky-header clearfix">
<a href="http://akka.io"><img class="logo" src="../images/akka-logo-reverse.svg"></a>

<button class="menu-icon float-right" type="button" data-toggle="underlay overlay"></button>
</div>
<div id="overlay" class="overlay-nav" data-toggler="nav-open">
<header class="nav-header">
<div class="nav-header-title">
<h1><a href="../index.html">Akka Documentation 中文</a></h1>
</div>
<div class="nav-header-version">
Version 2.5-SNAPSHOT
</div>
<div class="nav-header-groups">
<select class="supergroup" name="Language"><option class="group" value="group-scala">Scala</option><option class="group" value="group-java">Java</option></select>
</div>
<div id="overlay-search-container" class="nav-header-search">
<input id="overlay-search" type="search" class="search" placeholder="Search"/>
</div>
</header>
<nav class="nav-toc">
<ul>
  <li><a href="../security/index.html" class="page">Security Announcements</a></li>
  <li><a href="../guide/index.html" class="page">Getting Started Guide</a></li>
  <li><a href="../general/index.html" class="page">General Concepts</a></li>
  <li><a href="../index-actors.html" class="page">Actors</a></li>
  <li><a href="../typed/index.html" class="page">Akka Typed</a></li>
  <li><a href="../index-cluster.html" class="page">Clustering</a></li>
  <li><a href="../stream/index.html" class="page">Streams</a>
  <ul>
    <li><a href="../stream/index.html#dependency" class="header">Dependency</a></li>
    <li><a href="../stream/stream-introduction.html" class="page">Introduction</a></li>
    <li><a href="../stream/stream-quickstart.html" class="page">Streams Quickstart Guide</a></li>
    <li><a href="../general/stream/stream-design.html" class="page">Design Principles behind Akka Streams</a></li>
    <li><a href="../stream/stream-flows-and-basics.html" class="page">Basics and working with Flows</a></li>
    <li><a href="../stream/stream-graphs.html" class="page">Working with Graphs</a></li>
    <li><a href="../stream/stream-composition.html" class="page">Modularity, Composition and Hierarchy</a></li>
    <li><a href="../stream/stream-rate.html" class="page">Buffers and working with rate</a></li>
    <li><a href="../stream/stream-dynamic.html" class="page">Dynamic stream handling</a></li>
    <li><a href="../stream/stream-customize.html" class="page">Custom stream processing</a></li>
    <li><a href="../stream/stream-integrations.html" class="page">Integration</a></li>
    <li><a href="../stream/stream-error.html" class="page">Error Handling in Streams</a></li>
    <li><a href="../stream/stream-io.html" class="page">Working with streaming IO</a></li>
    <li><a href="../stream/stream-refs.html" class="page">StreamRefs - Reactive Streams over the network</a></li>
    <li><a href="../stream/stream-parallelism.html" class="page">Pipelining and Parallelism</a></li>
    <li><a href="../stream/stream-testkit.html" class="page">Testing streams</a></li>
    <li><a href="../stream/stream-substream.html" class="page">Substreams</a></li>
    <li><a href="../stream/stream-cookbook.html#streams-cookbook" class="active page">Streams Cookbook</a>
    <ul>
      <li><a href="../stream/stream-cookbook.html#dependency" class="header">Dependency</a></li>
      <li><a href="../stream/stream-cookbook.html#introduction" class="header">Introduction</a></li>
      <li><a href="../stream/stream-cookbook.html#working-with-flows" class="header">Working with Flows</a></li>
      <li><a href="../stream/stream-cookbook.html#working-with-operators" class="header">Working with Operators</a></li>
      <li><a href="../stream/stream-cookbook.html#working-with-rate" class="header">Working with rate</a></li>
      <li><a href="../stream/stream-cookbook.html#working-with-io" class="header">Working with IO</a></li>
    </ul></li>
    <li><a href="../general/stream/stream-configuration.html" class="page">Configuration</a></li>
    <li><a href="../stream/operators/index.html" class="page">Operators</a></li>
  </ul></li>
  <li><a href="../index-network.html" class="page">Networking</a></li>
  <li><a href="../discovery/index.html" class="page">Discovery</a></li>
  <li><a href="../index-futures.html" class="page">Futures and Agents</a></li>
  <li><a href="../index-utilities.html" class="page">Utilities</a></li>
  <li><a href="../common/other-modules.html" class="page">Other Akka modules</a></li>
  <li><a href="../howto.html" class="page">HowTo: Common Patterns</a></li>
  <li><a href="../project/index.html" class="page">Project Information</a></li>
  <li><a href="../additional/index.html" class="page">Additional Information</a></li>
  <li><a href="../chinese/index.html" class="page">中文版文档说明</a></li>
</ul>
</nav>
</div>
</header>

<aside class="show-for-large">
<header class="nav-header fixed-sidebar-header">
<div class="nav-header-title">
<h1><a href="../index.html">Akka Documentation 中文</a></h1>
</div>
<div class="nav-header-version">
Version 2.5-SNAPSHOT
</div>
<div class="nav-header-groups">
<select class="supergroup" name="Language"><option class="group" value="group-scala">Scala</option><option class="group" value="group-java">Java</option></select>
</div>
<div class="nav-header-search">
<input id="search" type="search" class="search" placeholder="Search"/>
</div>
</header>
<nav class="site-nav fixed-sidebar-contents">
<div class="nav-toc">
<ul>
  <li><a href="../security/index.html" class="page">Security Announcements</a></li>
  <li><a href="../guide/index.html" class="page">Getting Started Guide</a></li>
  <li><a href="../general/index.html" class="page">General Concepts</a></li>
  <li><a href="../index-actors.html" class="page">Actors</a></li>
  <li><a href="../typed/index.html" class="page">Akka Typed</a></li>
  <li><a href="../index-cluster.html" class="page">Clustering</a></li>
  <li><a href="../stream/index.html" class="page">Streams</a>
  <ul>
    <li><a href="../stream/index.html#dependency" class="header">Dependency</a></li>
    <li><a href="../stream/stream-introduction.html" class="page">Introduction</a></li>
    <li><a href="../stream/stream-quickstart.html" class="page">Streams Quickstart Guide</a></li>
    <li><a href="../general/stream/stream-design.html" class="page">Design Principles behind Akka Streams</a></li>
    <li><a href="../stream/stream-flows-and-basics.html" class="page">Basics and working with Flows</a></li>
    <li><a href="../stream/stream-graphs.html" class="page">Working with Graphs</a></li>
    <li><a href="../stream/stream-composition.html" class="page">Modularity, Composition and Hierarchy</a></li>
    <li><a href="../stream/stream-rate.html" class="page">Buffers and working with rate</a></li>
    <li><a href="../stream/stream-dynamic.html" class="page">Dynamic stream handling</a></li>
    <li><a href="../stream/stream-customize.html" class="page">Custom stream processing</a></li>
    <li><a href="../stream/stream-integrations.html" class="page">Integration</a></li>
    <li><a href="../stream/stream-error.html" class="page">Error Handling in Streams</a></li>
    <li><a href="../stream/stream-io.html" class="page">Working with streaming IO</a></li>
    <li><a href="../stream/stream-refs.html" class="page">StreamRefs - Reactive Streams over the network</a></li>
    <li><a href="../stream/stream-parallelism.html" class="page">Pipelining and Parallelism</a></li>
    <li><a href="../stream/stream-testkit.html" class="page">Testing streams</a></li>
    <li><a href="../stream/stream-substream.html" class="page">Substreams</a></li>
    <li><a href="../stream/stream-cookbook.html#streams-cookbook" class="active page">Streams Cookbook</a>
    <ul>
      <li><a href="../stream/stream-cookbook.html#dependency" class="header">Dependency</a></li>
      <li><a href="../stream/stream-cookbook.html#introduction" class="header">Introduction</a></li>
      <li><a href="../stream/stream-cookbook.html#working-with-flows" class="header">Working with Flows</a></li>
      <li><a href="../stream/stream-cookbook.html#working-with-operators" class="header">Working with Operators</a></li>
      <li><a href="../stream/stream-cookbook.html#working-with-rate" class="header">Working with rate</a></li>
      <li><a href="../stream/stream-cookbook.html#working-with-io" class="header">Working with IO</a></li>
    </ul></li>
    <li><a href="../general/stream/stream-configuration.html" class="page">Configuration</a></li>
    <li><a href="../stream/operators/index.html" class="page">Operators</a></li>
  </ul></li>
  <li><a href="../index-network.html" class="page">Networking</a></li>
  <li><a href="../discovery/index.html" class="page">Discovery</a></li>
  <li><a href="../index-futures.html" class="page">Futures and Agents</a></li>
  <li><a href="../index-utilities.html" class="page">Utilities</a></li>
  <li><a href="../common/other-modules.html" class="page">Other Akka modules</a></li>
  <li><a href="../howto.html" class="page">HowTo: Common Patterns</a></li>
  <li><a href="../project/index.html" class="page">Project Information</a></li>
  <li><a href="../additional/index.html" class="page">Additional Information</a></li>
  <li><a href="../chinese/index.html" class="page">中文版文档说明</a></li>
</ul>
</div>
</nav>
<footer class="nav-footer fixed-sidebar-footer">
<a href="http://akka.io"><img class="logo" src="../images/akka-logo-reverse.svg"></a>

</footer>
</aside>

<main class="fixed-shift-for-large expanded row">
<section class="site-content small-12 column">

<article class="page-content row">
<div class="small-12 large-9 column" id="docs">
<h1><a href="#streams-cookbook" name="streams-cookbook" class="anchor"><span class="anchor-link"></span></a>Streams Cookbook</h1>
<h2><a href="#dependency" name="dependency" class="anchor"><span class="anchor-link"></span></a>Dependency</h2>
<p>To use Akka Streams, add the module to your project:</p><dl class="dependency"><dt>sbt</dt><dd><pre class="prettyprint"><code class="language-scala">libraryDependencies += "com.typesafe.akka" %% "akka-stream" % "2.5-SNAPSHOT"</code></pre></dd><dt>Maven</dt><dd><pre class="prettyprint"><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt;
  &lt;artifactId&gt;akka-stream_2.12&lt;/artifactId&gt;
  &lt;version&gt;2.5-SNAPSHOT&lt;/version&gt;
&lt;/dependency&gt;</code></pre></dd><dt>Gradle</dt><dd><pre class="prettyprint"><code class="language-gradle">dependencies {
  compile group: 'com.typesafe.akka', name: 'akka-stream_2.12', version: '2.5-SNAPSHOT'
}</code></pre></dd></dl>
<h2><a href="#introduction" name="introduction" class="anchor"><span class="anchor-link"></span></a>Introduction</h2>
<p>This is a collection of patterns to demonstrate various usage of the Akka Streams API by solving small targeted problems in the format of &ldquo;recipes&rdquo;. The purpose of this page is to give inspiration and ideas how to approach various small tasks involving streams. The recipes in this page can be used directly as-is, but they are most powerful as starting points: customization of the code snippets is warmly encouraged.</p>
<p>This part also serves as supplementary material for the main body of documentation. It is a good idea to have this page open while reading the manual and look for examples demonstrating various streaming concepts as they appear in the main body of documentation.</p>
<p>If you need a quick reference of the available operators used in the recipes see <a href="operators/index.html">operator index</a>.</p>
<h2><a href="#working-with-flows" name="working-with-flows" class="anchor"><span class="anchor-link"></span></a>Working with Flows</h2>
<p>In this collection we show simple recipes that involve linear flows. The recipes in this section are rather general, more targeted recipes are available as separate sections (<a href="stream-rate.html">Buffers and working with rate</a>, <a href="stream-io.html">Working with streaming IO</a>).</p>
<h3><a href="#logging-in-streams" name="logging-in-streams" class="anchor"><span class="anchor-link"></span></a>Logging in streams</h3>
<p><strong>Situation:</strong> During development it is sometimes helpful to see what happens in a particular section of a stream.</p>
<p>The simplest solution is to use a <code>map</code> operation and use <code>println</code> to print the elements received to the console. While this recipe is rather simplistic, it is often suitable for a quick debug session.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeLoggingElements.scala#L23" target="_blank" title="Go to snippet source"></a><code class="language-scala">val loggedSource = mySource.map { elem ⇒ println(elem); elem }</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeLoggingElements.java#L57-L61" target="_blank" title="Go to snippet source"></a><code class="language-java">mySource.map(
    elem -&gt; {
      System.out.println(elem);
      return elem;
    });</code></pre></dd>
</dl>
<p>Another approach to logging is to use <code>log()</code> operation. This approach gives you more fine-grained control of logging levels for elements flowing through the stream, finish and failure of the stream.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeLoggingElements.scala#L35-L48" target="_blank" title="Go to snippet source"></a><code class="language-scala">// customise log levels
mySource.log(&quot;before-map&quot;)
  .withAttributes(
    Attributes.logLevels(
      onElement = Logging.WarningLevel,
      onFinish = Logging.InfoLevel,
      onFailure = Logging.DebugLevel
    )
  )
  .map(analyse)

// or provide custom logging adapter
implicit val adapter = Logging(system, &quot;customLogger&quot;)
mySource.log(&quot;custom&quot;)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeLoggingElements.java#L78-L91" target="_blank" title="Go to snippet source"></a><code class="language-java">// customise log levels
mySource
    .log(&quot;before-map&quot;)
    .withAttributes(
        Attributes.createLogLevels(
            Logging.WarningLevel(), // onElement
            Logging.InfoLevel(), // onFinish
            Logging.DebugLevel() // onFailure
            ))
    .map(i -&gt; analyse(i));

// or provide custom logging adapter
final LoggingAdapter adapter = Logging.getLogger(system, &quot;customLogger&quot;);
mySource.log(&quot;custom&quot;, adapter);</code></pre></dd>
</dl>
<h3><a href="#creating-a-source-that-continuously-evaluates-a-function" name="creating-a-source-that-continuously-evaluates-a-function" class="anchor"><span class="anchor-link"></span></a>Creating a source that continuously evaluates a function</h3>
<p><strong>Situation:</strong> A source is required that continuously provides elements obtained by evaluating a given function, so long as there is demand.</p>
<p>The simplest implementation is to use a <code>Source.repeat</code> that produces some arbitrary element - e.g. <code>NotUsed</code> - and then map those elements to the function evaluation. E.g. if we have some <code>builderFunction()</code>, we can use:</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeSourceFromFunction.scala#L20" target="_blank" title="Go to snippet source"></a><code class="language-scala">val source = Source.repeat(NotUsed).map(_ ⇒ builderFunction())</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeSourceFromFunction.java#L54-L55" target="_blank" title="Go to snippet source"></a><code class="language-java">final Source&lt;String, NotUsed&gt; source =
    Source.repeat(NotUsed.getInstance()).map(elem -&gt; builderFunction());</code></pre></dd>
</dl>
<p>Note: if the element-builder function touches mutable state, then a guaranteed single-threaded source should be used instead; e.g. <code>Source.unfold</code> or <code>Source.unfoldResource</code>.</p>
<h3><a href="#flattening-a-stream-of-sequences" name="flattening-a-stream-of-sequences" class="anchor"><span class="anchor-link"></span></a>Flattening a stream of sequences</h3>
<p><strong>Situation:</strong> A stream is given as a stream of sequence of elements, but a stream of elements needed instead, streaming all the nested elements inside the sequences separately.</p>
<p>The <code>mapConcat</code> operation can be used to implement a one-to-many transformation of elements using a mapper function in the form of <span class="group-scala"><code>In =&gt; immutable.Seq[Out]</code></span> <span class="group-java"><code>In -&gt; List&lt;Out&gt;</code></span>. In this case we want to map a <span class="group-scala"><code>Seq</code></span> <span class="group-java"><code>List</code></span> of elements to the elements in the collection itself, so we can call <span class="group-scala"><code>mapConcat(identity)</code></span> <span class="group-java"><code>mapConcat(l -&gt; l)</code></span>.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeFlattenSeq.scala#L22-L23" target="_blank" title="Go to snippet source"></a><code class="language-scala">val myData: Source[List[Message], NotUsed] = someDataSource
val flattened: Source[Message, NotUsed] = myData.mapConcat(identity)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeFlattenList.java#L52-L53" target="_blank" title="Go to snippet source"></a><code class="language-java">Source&lt;List&lt;Message&gt;, NotUsed&gt; myData = someDataSource;
Source&lt;Message, NotUsed&gt; flattened = myData.mapConcat(i -&gt; i);</code></pre></dd>
</dl>
<h3><a href="#draining-a-stream-to-a-strict-collection" name="draining-a-stream-to-a-strict-collection" class="anchor"><span class="anchor-link"></span></a>Draining a stream to a strict collection</h3>
<p><strong>Situation:</strong> A possibly unbounded sequence of elements is given as a stream, which needs to be collected into a Scala collection while ensuring boundedness</p>
<p>A common situation when working with streams is one where we need to collect incoming elements into a Scala collection. This operation is supported via <code>Sink.seq</code> which materializes into a <span class="group-scala"><code>Future[Seq[T]]</code></span> <span class="group-java"><code>CompletionStage&lt;List&lt;T&gt;&gt;</code></span>.</p>
<p>The function <code>limit</code> or <code>take</code> should always be used in conjunction in order to guarantee stream boundedness, thus preventing the program from running out of memory.</p>
<p>For example, this is best avoided:</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeSeq.scala#L17-L18" target="_blank" title="Go to snippet source"></a><code class="language-scala">// Dangerous: might produce a collection with 2 billion elements!
val f: Future[Seq[String]] = mySource.runWith(Sink.seq)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeSeq.java#L46-L47" target="_blank" title="Go to snippet source"></a><code class="language-java">// Dangerous: might produce a collection with 2 billion elements!
final CompletionStage&lt;List&lt;String&gt;&gt; strings = mySource.runWith(Sink.seq(), mat);</code></pre></dd>
</dl>
<p>Rather, use <code>limit</code> or <code>take</code> to ensure that the resulting <span class="group-scala"><code>Seq</code></span> <span class="group-java"><code>List</code></span> will contain only up to <span class="group-scala"><code>max</code></span> <span class="group-java"><code>MAX_ALLOWED_SIZE</code></span> elements:</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeSeq.scala#L26-L35" target="_blank" title="Go to snippet source"></a><code class="language-scala">val MAX_ALLOWED_SIZE = 100

// OK. Future will fail with a `StreamLimitReachedException`
// if the number of incoming elements is larger than max
val limited: Future[Seq[String]] =
  mySource.limit(MAX_ALLOWED_SIZE).runWith(Sink.seq)

// OK. Collect up until max-th elements only, then cancel upstream
val ignoreOverflow: Future[Seq[String]] =
  mySource.take(MAX_ALLOWED_SIZE).runWith(Sink.seq)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeSeq.java#L61-L84" target="_blank" title="Go to snippet source"></a><code class="language-java">final int MAX_ALLOWED_SIZE = 100;

// OK. Future will fail with a `StreamLimitReachedException`
// if the number of incoming elements is larger than max
final CompletionStage&lt;List&lt;String&gt;&gt; strings =
    mySource.limit(MAX_ALLOWED_SIZE).runWith(Sink.seq(), mat);

// OK. Collect up until max-th elements only, then cancel upstream
final CompletionStage&lt;List&lt;String&gt;&gt; strings =
    mySource.take(MAX_ALLOWED_SIZE).runWith(Sink.seq(), mat);</code></pre></dd>
</dl>
<h3><a href="#calculating-the-digest-of-a-bytestring-stream" name="calculating-the-digest-of-a-bytestring-stream" class="anchor"><span class="anchor-link"></span></a>Calculating the digest of a ByteString stream</h3>
<p><strong>Situation:</strong> A stream of bytes is given as a stream of <code>ByteString</code> s and we want to calculate the cryptographic digest of the stream.</p>
<p>This recipe uses a <a href="stream-customize.html"><code>GraphStage</code></a> to define a custom Akka Stream operator, to host a mutable <code>MessageDigest</code> class (part of the Java Cryptography API) and update it with the bytes arriving from the stream. When the stream starts, the <code>onPull</code> handler of the operator is called, which bubbles up the <code>pull</code> event to its upstream. As a response to this pull, a ByteString chunk will arrive (<code>onPush</code>) which we use to update the digest, then it will pull for the next chunk.</p>
<p>Eventually the stream of <code>ByteString</code> s depletes and we get a notification about this event via <code>onUpstreamFinish</code>. At this point we want to emit the digest value, but we cannot do it with <code>push</code> in this handler directly since there may be no downstream demand. Instead we call <code>emit</code> which will temporarily replace the handlers, emit the provided value when demand comes in and then reset the operator state. It will then complete the operator.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeDigest.scala#L17-L55" target="_blank" title="Go to snippet source"></a><code class="language-scala">import java.security.MessageDigest

import akka.NotUsed
import akka.stream.{ Attributes, Outlet, Inlet, FlowShape }
import akka.stream.scaladsl.{ Sink, Source }
import akka.util.ByteString

import akka.stream.stage._

val data: Source[ByteString, NotUsed] = Source.single(ByteString(&quot;abc&quot;))

class DigestCalculator(algorithm: String) extends GraphStage[FlowShape[ByteString, ByteString]] {
  val in = Inlet[ByteString](&quot;DigestCalculator.in&quot;)
  val out = Outlet[ByteString](&quot;DigestCalculator.out&quot;)
  override val shape = FlowShape(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {
    private val digest = MessageDigest.getInstance(algorithm)

    setHandler(out, new OutHandler {
      override def onPull(): Unit = pull(in)
    })

    setHandler(in, new InHandler {
      override def onPush(): Unit = {
        val chunk = grab(in)
        digest.update(chunk.toArray)
        pull(in)
      }

      override def onUpstreamFinish(): Unit = {
        emit(out, ByteString(digest.digest()))
        completeStage()
      }
    })
  }
}

val digest: Source[ByteString, NotUsed] = data.via(new DigestCalculator(&quot;SHA-256&quot;))</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeDigest.java#L43-L100" target="_blank" title="Go to snippet source"></a><code class="language-java">class DigestCalculator extends GraphStage&lt;FlowShape&lt;ByteString, ByteString&gt;&gt; {
  private final String algorithm;
  public Inlet&lt;ByteString&gt; in = Inlet.create(&quot;DigestCalculator.in&quot;);
  public Outlet&lt;ByteString&gt; out = Outlet.create(&quot;DigestCalculator.out&quot;);
  private FlowShape&lt;ByteString, ByteString&gt; shape = FlowShape.of(in, out);

  public DigestCalculator(String algorithm) {
    this.algorithm = algorithm;
  }

  @Override
  public FlowShape&lt;ByteString, ByteString&gt; shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {
      final MessageDigest digest;

      {
        try {
          digest = MessageDigest.getInstance(algorithm);
        } catch (NoSuchAlgorithmException ex) {
          throw new RuntimeException(ex);
        }

        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() {
                pull(in);
              }
            });

        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() {
                ByteString chunk = grab(in);
                digest.update(chunk.toArray());
                pull(in);
              }

              @Override
              public void onUpstreamFinish() {
                // If the stream is finished, we need to emit the digest
                // before completing
                emit(out, ByteString.fromArray(digest.digest()));
                completeStage();
              }
            });
      }
    };
  }
}</code></pre></dd>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeDigest.java#L110" target="_blank" title="Go to snippet source"></a><code class="language-java">final Source&lt;ByteString, NotUsed&gt; digest = data.via(new DigestCalculator(&quot;SHA-256&quot;));</code></pre></dd>
</dl>
<a id="cookbook-parse-lines"></a>
<h3><a href="#parsing-lines-from-a-stream-of-bytestrings" name="parsing-lines-from-a-stream-of-bytestrings" class="anchor"><span class="anchor-link"></span></a>Parsing lines from a stream of ByteStrings</h3>
<p><strong>Situation:</strong> A stream of bytes is given as a stream of <code>ByteString</code> s containing lines terminated by line ending characters (or, alternatively, containing binary frames delimited by a special delimiter byte sequence) which needs to be parsed.</p>
<p>The <code>Framing</code> helper <span class="group-scala">object</span> <span class="group-java">class</span> contains a convenience method to parse messages from a stream of <code>ByteString</code> s:</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeParseLines.scala#L27-L30" target="_blank" title="Go to snippet source"></a><code class="language-scala">import akka.stream.scaladsl.Framing
val linesStream = rawData.via(Framing.delimiter(
  ByteString(&quot;\r\n&quot;), maximumFrameLength = 100, allowTruncation = true))
  .map(_.utf8String)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeParseLines.java#L54-L57" target="_blank" title="Go to snippet source"></a><code class="language-java">final Source&lt;String, NotUsed&gt; lines =
    rawData
        .via(Framing.delimiter(ByteString.fromString(&quot;\r\n&quot;), 100, FramingTruncation.ALLOW))
        .map(b -&gt; b.utf8String());</code></pre></dd>
</dl>
<h3><a href="#dealing-with-compressed-data-streams" name="dealing-with-compressed-data-streams" class="anchor"><span class="anchor-link"></span></a>Dealing with compressed data streams</h3>
<p><strong>Situation:</strong> A gzipped stream of bytes is given as a stream of <code>ByteString</code> s, for example from a <code>FileIO</code> source.</p>
<p>The <code>Compression</code> helper <span class="group-scala">object</span> <span class="group-java">class</span> contains convenience methods for decompressing data streams compressed with Gzip or Deflate.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeDecompress.scala#L18-L27" target="_blank" title="Go to snippet source"></a><code class="language-scala">import akka.stream.scaladsl.Compression
val uncompressed = compressed.via(Compression.gunzip())
  .map(_.utf8String)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeDecompress.java#L48-L49" target="_blank" title="Go to snippet source"></a><code class="language-java">final Source&lt;ByteString, NotUsed&gt; decompressedStream =
    compressedStream.via(Compression.gunzip(100));</code></pre></dd>
</dl>
<h3><a href="#implementing-reduce-by-key" name="implementing-reduce-by-key" class="anchor"><span class="anchor-link"></span></a>Implementing reduce-by-key</h3>
<p><strong>Situation:</strong> Given a stream of elements, we want to calculate some aggregated value on different subgroups of the elements.</p>
<p>The &ldquo;hello world&rdquo; of reduce-by-key style operations is <em>wordcount</em> which we demonstrate below. Given a stream of words we first create a new stream that groups the words according to the <span class="group-scala"><code>identity</code></span> <span class="group-java"><code>i -&gt; i</code></span> function, i.e. now we have a stream of streams, where every substream will serve identical words.</p>
<p>To count the words, we need to process the stream of streams (the actual groups containing identical words). <code>groupBy</code> returns a <span class="group-scala"><code>SubFlow</code></span> <span class="group-java"><code>SubSource</code></span>, which means that we transform the resulting substreams directly. In this case we use the <code>reduce</code> operator to aggregate the word itself and the number of its occurrences within a <span class="group-scala">tuple <code>(String, Integer)</code></span> <span class="group-java"><code>Pair&lt;String, Integer&gt;</code></span>. Each substream will then emit one final value—precisely such a pair—when the overall input completes. As a last step we merge back these values from the substreams into one single output stream.</p>
<p>One noteworthy detail pertains to the <span class="group-scala"><code>MaximumDistinctWords</code></span> <span class="group-java"><code>MAXIMUM_DISTINCT_WORDS</code></span> parameter: this defines the breadth of the groupBy and merge operations. Akka Streams is focused on bounded resource consumption and the number of concurrently open inputs to the merge operator describes the amount of resources needed by the merge itself. Therefore only a finite number of substreams can be active at any given time. If the <code>groupBy</code> operator encounters more keys than this number then the stream cannot continue without violating its resource bound, in this case <code>groupBy</code> will terminate with a failure.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeReduceByKey.scala#L23-L31" target="_blank" title="Go to snippet source"></a><code class="language-scala">val counts: Source[(String, Int), NotUsed] = words
  // split the words into separate streams first
  .groupBy(MaximumDistinctWords, identity)
  //transform each element to pair with number of words in it
  .map(_ -&gt; 1)
  // add counting logic to the streams
  .reduce((l, r) ⇒ (l._1, l._2 + r._2))
  // get a stream of word counts
  .mergeSubstreams</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeReduceByKeyTest.java#L56-L67" target="_blank" title="Go to snippet source"></a><code class="language-java">final int MAXIMUM_DISTINCT_WORDS = 1000;

final Source&lt;Pair&lt;String, Integer&gt;, NotUsed&gt; counts =
    words
        // split the words into separate streams first
        .groupBy(MAXIMUM_DISTINCT_WORDS, i -&gt; i)
        // transform each element to pair with number of words in it
        .map(i -&gt; new Pair&lt;&gt;(i, 1))
        // add counting logic to the streams
        .reduce((left, right) -&gt; new Pair&lt;&gt;(left.first(), left.second() + right.second()))
        // get a stream of word counts
        .mergeSubstreams();</code></pre></dd>
</dl>
<p>By extracting the parts specific to <em>wordcount</em> into</p>
<ul>
  <li>a <code>groupKey</code> function that defines the groups</li>
  <li>a <code>map</code> map each element to value that is used by the reduce on the substream</li>
  <li>a <code>reduce</code> function that does the actual reduction</li>
</ul>
<p>we get a generalized version below:</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeReduceByKey.scala#L48-L64" target="_blank" title="Go to snippet source"></a><code class="language-scala">def reduceByKey[In, K, Out](
  maximumGroupSize: Int,
  groupKey:         (In) ⇒ K,
  map:              (In) ⇒ Out)(reduce: (Out, Out) ⇒ Out): Flow[In, (K, Out), NotUsed] = {

  Flow[In]
    .groupBy[K](maximumGroupSize, groupKey)
    .map(e ⇒ groupKey(e) -&gt; map(e))
    .reduce((l, r) ⇒ l._1 -&gt; reduce(l._2, r._2))
    .mergeSubstreams
}

val wordCounts = words.via(
  reduceByKey(
    MaximumDistinctWords,
    groupKey = (word: String) ⇒ word,
    map = (word: String) ⇒ 1)((left: Int, right: Int) ⇒ left + right))</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeReduceByKeyTest.java#L85-L97" target="_blank" title="Go to snippet source"></a><code class="language-java">public static &lt;In, K, Out&gt; Flow&lt;In, Pair&lt;K, Out&gt;, NotUsed&gt; reduceByKey(
    int maximumGroupSize,
    Function&lt;In, K&gt; groupKey,
    Function&lt;In, Out&gt; map,
    Function2&lt;Out, Out, Out&gt; reduce) {

  return Flow.&lt;In&gt;create()
      .groupBy(maximumGroupSize, groupKey)
      .map(i -&gt; new Pair&lt;&gt;(groupKey.apply(i), map.apply(i)))
      .reduce(
          (left, right) -&gt; new Pair&lt;&gt;(left.first(), reduce.apply(left.second(), right.second())))
      .mergeSubstreams();
}</code></pre></dd>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeReduceByKeyTest.java#L108-L117" target="_blank" title="Go to snippet source"></a><code class="language-java">final int MAXIMUM_DISTINCT_WORDS = 1000;

Source&lt;Pair&lt;String, Integer&gt;, NotUsed&gt; counts =
    words.via(
        reduceByKey(
            MAXIMUM_DISTINCT_WORDS,
            word -&gt; word,
            word -&gt; 1,
            (left, right) -&gt; left + right));
</code></pre></dd>
</dl><div class="callout note "><div class="callout-title">Note</div>
<p>Please note that the reduce-by-key version we discussed above is sequential in reading the overall input stream, in other words it is <strong>NOT</strong> a parallelization pattern like MapReduce and similar frameworks.</p></div>
<h3><a href="#sorting-elements-to-multiple-groups-with-groupby" name="sorting-elements-to-multiple-groups-with-groupby" class="anchor"><span class="anchor-link"></span></a>Sorting elements to multiple groups with groupBy</h3>
<p><strong>Situation:</strong> The <code>groupBy</code> operation strictly partitions incoming elements, each element belongs to exactly one group. Sometimes we want to map elements into multiple groups simultaneously.</p>
<p>To achieve the desired result, we attack the problem in two steps:</p>
<ul>
  <li>first, using a function <code>topicMapper</code> that gives a list of topics (groups) a message belongs to, we transform our stream of <code>Message</code> to a stream of <span class="group-scala"><code>(Message, Topic)</code></span> <span class="group-java"><code>Pair&lt;Message, Topic&gt;</code></span> where for each topic the message belongs to a separate pair will be emitted. This is achieved by using <code>mapConcat</code></li>
  <li>Then we take this new stream of message topic pairs (containing a separate pair for each topic a given message belongs to) and feed it into groupBy, using the topic as the group key.</li>
</ul>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeMultiGroupBy.scala#L29-L45" target="_blank" title="Go to snippet source"></a><code class="language-scala">val topicMapper: (Message) ⇒ immutable.Seq[Topic] = extractTopics

val messageAndTopic: Source[(Message, Topic), NotUsed] = elems.mapConcat { msg: Message ⇒
  val topicsForMessage = topicMapper(msg)
  // Create a (Msg, Topic) pair for each of the topics
  // the message belongs to
  topicsForMessage.map(msg -&gt; _)
}

val multiGroups = messageAndTopic
  .groupBy(2, _._2).map {
    case (msg, topic) ⇒
      // do what needs to be done
  }</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeMultiGroupByTest.java#L100-L127" target="_blank" title="Go to snippet source"></a><code class="language-java">final Function&lt;Message, List&lt;Topic&gt;&gt; topicMapper = m -&gt; extractTopics(m);

final Source&lt;Pair&lt;Message, Topic&gt;, NotUsed&gt; messageAndTopic =
    elems.mapConcat(
        (Message msg) -&gt; {
          List&lt;Topic&gt; topicsForMessage = topicMapper.apply(msg);
          // Create a (Msg, Topic) pair for each of the topics

          // the message belongs to
          return topicsForMessage
              .stream()
              .map(topic -&gt; new Pair&lt;Message, Topic&gt;(msg, topic))
              .collect(toList());
        });

SubSource&lt;Pair&lt;Message, Topic&gt;, NotUsed&gt; multiGroups =
    messageAndTopic
        .groupBy(2, pair -&gt; pair.second())
        .map(
            pair -&gt; {
              Message message = pair.first();
              Topic topic = pair.second();

              // do what needs to be done
            });</code></pre></dd>
</dl>
<h3><a href="#adhoc-source" name="adhoc-source" class="anchor"><span class="anchor-link"></span></a>Adhoc source</h3>
<p><strong>Situation:</strong> The idea is that you have a source which you don&rsquo;t want to start until you have a demand. Also, you want to shutdown it down when there is no more demand, and start it up again there is new demand again.</p>
<p>You can achieve this behavior by combining <code>lazily</code>, <code>backpressureTimeout</code> and <code>recoverWithRetries</code> as follows:</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeAdhocSource.scala#L20-L26" target="_blank" title="Go to snippet source"></a><code class="language-scala">def adhocSource[T](source: Source[T, _], timeout: FiniteDuration, maxRetries: Int): Source[T, _] =
  Source.lazily(
    () ⇒ source.backpressureTimeout(timeout).recoverWithRetries(maxRetries, {
      case t: TimeoutException ⇒
        Source.lazily(() ⇒ source.backpressureTimeout(timeout)).mapMaterializedValue(_ ⇒ NotUsed)
    })
  )</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeAdhocSourceTest.java#L52-L64" target="_blank" title="Go to snippet source"></a><code class="language-java">public &lt;T&gt; Source&lt;T, ?&gt; adhocSource(Source&lt;T, ?&gt; source, Duration timeout, int maxRetries) {
  return Source.lazily(
      () -&gt;
          source
              .backpressureTimeout(timeout)
              .recoverWithRetries(
                  maxRetries,
                  new PFBuilder()
                      .match(
                          TimeoutException.class,
                          ex -&gt; Source.lazily(() -&gt; source.backpressureTimeout(timeout)))
                      .build()));
}</code></pre></dd>
</dl>
<h2><a href="#working-with-operators" name="working-with-operators" class="anchor"><span class="anchor-link"></span></a>Working with Operators</h2>
<p>In this collection we show recipes that use stream operators to achieve various goals.</p>
<h3><a href="#triggering-the-flow-of-elements-programmatically" name="triggering-the-flow-of-elements-programmatically" class="anchor"><span class="anchor-link"></span></a>Triggering the flow of elements programmatically</h3>
<p><strong>Situation:</strong> Given a stream of elements we want to control the emission of those elements according to a trigger signal. In other words, even if the stream would be able to flow (not being backpressured) we want to hold back elements until a trigger signal arrives.</p>
<p>This recipe solves the problem by zipping the stream of <code>Message</code> elements with the stream of <code>Trigger</code> signals. Since <code>Zip</code> produces pairs, we map the output stream selecting the first element of the pair.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeManualTrigger.scala#L26-L33" target="_blank" title="Go to snippet source"></a><code class="language-scala">val graph = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder ⇒
  import GraphDSL.Implicits._
  val zip = builder.add(Zip[Message, Trigger]())
  elements ~&gt; zip.in0
  triggerSource ~&gt; zip.in1
  zip.out ~&gt; Flow[(Message, Trigger)].map { case (msg, trigger) ⇒ msg } ~&gt; sink
  ClosedShape
})</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeManualTrigger.java#L54-L75" target="_blank" title="Go to snippet source"></a><code class="language-java">final RunnableGraph&lt;Pair&lt;TestPublisher.Probe&lt;Trigger&gt;, TestSubscriber.Probe&lt;Message&gt;&gt;&gt; g =
    RunnableGraph
        .&lt;Pair&lt;TestPublisher.Probe&lt;Trigger&gt;, TestSubscriber.Probe&lt;Message&gt;&gt;&gt;fromGraph(
            GraphDSL.create(
                triggerSource,
                messageSink,
                (p, s) -&gt; new Pair&lt;&gt;(p, s),
                (builder, source, sink) -&gt; {
                  SourceShape&lt;Message&gt; elements =
                      builder.add(
                          Source.from(Arrays.asList(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;))
                              .map(t -&gt; new Message(t)));
                  FlowShape&lt;Pair&lt;Message, Trigger&gt;, Message&gt; takeMessage =
                      builder.add(
                          Flow.&lt;Pair&lt;Message, Trigger&gt;&gt;create().map(p -&gt; p.first()));
                  final FanInShape2&lt;Message, Trigger, Pair&lt;Message, Trigger&gt;&gt; zip =
                      builder.add(Zip.create());
                  builder.from(elements).toInlet(zip.in0());
                  builder.from(source).toInlet(zip.in1());
                  builder.from(zip.out()).via(takeMessage).to(sink);
                  return ClosedShape.getInstance();
                }));</code></pre></dd>
</dl>
<p>Alternatively, instead of using a <code>Zip</code>, and then using <code>map</code> to get the first element of the pairs, we can avoid creating the pairs in the first place by using <code>ZipWith</code> which takes a two argument function to produce the output element. If this function would return a pair of the two argument it would be exactly the behavior of <code>Zip</code> so <code>ZipWith</code> is a generalization of zipping.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeManualTrigger.scala#L65-L73" target="_blank" title="Go to snippet source"></a><code class="language-scala">val graph = RunnableGraph.fromGraph(GraphDSL.create() { implicit builder ⇒
  import GraphDSL.Implicits._
  val zip = builder.add(ZipWith((msg: Message, trigger: Trigger) ⇒ msg))

  elements ~&gt; zip.in0
  triggerSource ~&gt; zip.in1
  zip.out ~&gt; sink
  ClosedShape
})</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeManualTrigger.java#L112-L130" target="_blank" title="Go to snippet source"></a><code class="language-java">final RunnableGraph&lt;Pair&lt;TestPublisher.Probe&lt;Trigger&gt;, TestSubscriber.Probe&lt;Message&gt;&gt;&gt; g =
    RunnableGraph
        .&lt;Pair&lt;TestPublisher.Probe&lt;Trigger&gt;, TestSubscriber.Probe&lt;Message&gt;&gt;&gt;fromGraph(
            GraphDSL.create(
                triggerSource,
                messageSink,
                (p, s) -&gt; new Pair&lt;&gt;(p, s),
                (builder, source, sink) -&gt; {
                  final SourceShape&lt;Message&gt; elements =
                      builder.add(
                          Source.from(Arrays.asList(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;))
                              .map(t -&gt; new Message(t)));
                  final FanInShape2&lt;Message, Trigger, Message&gt; zipWith =
                      builder.add(ZipWith.create((msg, trigger) -&gt; msg));
                  builder.from(elements).toInlet(zipWith.in0());
                  builder.from(source).toInlet(zipWith.in1());
                  builder.from(zipWith.out()).to(sink);
                  return ClosedShape.getInstance();
                }));</code></pre></dd>
</dl>
<a id="cookbook-balance"></a>
<h3><a href="#balancing-jobs-to-a-fixed-pool-of-workers" name="balancing-jobs-to-a-fixed-pool-of-workers" class="anchor"><span class="anchor-link"></span></a>Balancing jobs to a fixed pool of workers</h3>
<p><strong>Situation:</strong> Given a stream of jobs and a worker process expressed as a <code>Flow</code> create a pool of workers that automatically balances incoming jobs to available workers, then merges the results.</p>
<p>We will express our solution as a function that takes a worker flow and the number of workers to be allocated and gives a flow that internally contains a pool of these workers. To achieve the desired result we will create a <code>Flow</code> from an operator.</p>
<p>The operator consists of a <code>Balance</code> node which is a special fan-out operation that tries to route elements to available downstream consumers. In a <code>for</code> loop we wire all of our desired workers as outputs of this balancer element, then we wire the outputs of these workers to a <code>Merge</code> element that will collect the results from the workers.</p>
<p>To make the worker operators run in parallel we mark them as asynchronous with <em>async</em>.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeWorkerPool.scala#L25-L42" target="_blank" title="Go to snippet source"></a><code class="language-scala">def balancer[In, Out](worker: Flow[In, Out, Any], workerCount: Int): Flow[In, Out, NotUsed] = {
  import GraphDSL.Implicits._

  Flow.fromGraph(GraphDSL.create() { implicit b ⇒
    val balancer = b.add(Balance[In](workerCount, waitForAllDownstreams = true))
    val merge = b.add(Merge[Out](workerCount))

    for (_ ← 1 to workerCount) {
      // for each worker, add an edge from the balancer to the worker, then wire
      // it to the merge element
      balancer ~&gt; worker.async ~&gt; merge
    }

    FlowShape(balancer.in, merge.out)
  })
}

val processedJobs: Source[Result, NotUsed] = myJobs.via(balancer(worker, 3))</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeWorkerPool.java#L42-L58" target="_blank" title="Go to snippet source"></a><code class="language-java">public static &lt;In, Out&gt; Flow&lt;In, Out, NotUsed&gt; balancer(
    Flow&lt;In, Out, NotUsed&gt; worker, int workerCount) {
  return Flow.fromGraph(
      GraphDSL.create(
          b -&gt; {
            boolean waitForAllDownstreams = true;
            final UniformFanOutShape&lt;In, In&gt; balance =
                b.add(Balance.&lt;In&gt;create(workerCount, waitForAllDownstreams));
            final UniformFanInShape&lt;Out, Out&gt; merge = b.add(Merge.&lt;Out&gt;create(workerCount));

            for (int i = 0; i &lt; workerCount; i++) {
              b.from(balance.out(i)).via(b.add(worker.async())).toInlet(merge.in(i));
            }

            return FlowShape.of(balance.in(), merge.out());
          }));
}</code></pre></dd>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeWorkerPool.java#L72-L73" target="_blank" title="Go to snippet source"></a><code class="language-java">Flow&lt;Message, Message, NotUsed&gt; balancer = balancer(worker, 3);
Source&lt;Message, NotUsed&gt; processedJobs = data.via(balancer);</code></pre></dd>
</dl>
<h2><a href="#working-with-rate" name="working-with-rate" class="anchor"><span class="anchor-link"></span></a>Working with rate</h2>
<p>This collection of recipes demonstrate various patterns where rate differences between upstream and downstream needs to be handled by other strategies than simple backpressure.</p>
<h3><a href="#dropping-elements" name="dropping-elements" class="anchor"><span class="anchor-link"></span></a>Dropping elements</h3>
<p><strong>Situation:</strong> Given a fast producer and a slow consumer, we want to drop elements if necessary to not slow down the producer too much.</p>
<p>This can be solved by using a versatile rate-transforming operation, <code>conflate</code>. Conflate can be thought as a special <code>reduce</code> operation that collapses multiple upstream elements into one aggregate element if needed to keep the speed of the upstream unaffected by the downstream.</p>
<p>When the upstream is faster, the reducing process of the <code>conflate</code> starts. Our reducer function takes the freshest element. This in a simple dropping operation.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeSimpleDrop.scala#L21-L22" target="_blank" title="Go to snippet source"></a><code class="language-scala">val droppyStream: Flow[Message, Message, NotUsed] =
  Flow[Message].conflate((lastMessage, newMessage) ⇒ newMessage)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeSimpleDrop.java#L50-L51" target="_blank" title="Go to snippet source"></a><code class="language-java">final Flow&lt;Message, Message, NotUsed&gt; droppyStream =
    Flow.of(Message.class).conflate((lastMessage, newMessage) -&gt; newMessage);</code></pre></dd>
</dl>
<p>There is a more general version of <code>conflate</code> named <code>conflateWithSeed</code> that allows to express more complex aggregations, more similar to a <code>fold</code>.</p>
<h3><a href="#dropping-broadcast" name="dropping-broadcast" class="anchor"><span class="anchor-link"></span></a>Dropping broadcast</h3>
<p><strong>Situation:</strong> The default <code>Broadcast</code> operator is properly backpressured, but that means that a slow downstream consumer can hold back the other downstream consumers resulting in lowered throughput. In other words the rate of <code>Broadcast</code> is the rate of its slowest downstream consumer. In certain cases it is desirable to allow faster consumers to progress independently of their slower siblings by dropping elements if necessary.</p>
<p>One solution to this problem is to append a <code>buffer</code> element in front of all of the downstream consumers defining a dropping strategy instead of the default <code>Backpressure</code>. This allows small temporary rate differences between the different consumers (the buffer smooths out small rate variances), but also allows faster consumers to progress by dropping from the buffer of the slow consumers if necessary.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeDroppyBroadcast.scala#L27-L37" target="_blank" title="Go to snippet source"></a><code class="language-scala">val graph = RunnableGraph.fromGraph(GraphDSL.create(mySink1, mySink2, mySink3)((_, _, _)) { implicit b ⇒ (sink1, sink2, sink3) ⇒
  import GraphDSL.Implicits._

  val bcast = b.add(Broadcast[Int](3))
  myElements ~&gt; bcast

  bcast.buffer(10, OverflowStrategy.dropHead) ~&gt; sink1
  bcast.buffer(10, OverflowStrategy.dropHead) ~&gt; sink2
  bcast.buffer(10, OverflowStrategy.dropHead) ~&gt; sink3
  ClosedShape
})</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeDroppyBroadcast.java#L42-L46" target="_blank" title="Go to snippet source"></a><code class="language-java">// Makes a sink drop elements if too slow
public &lt;T&gt; Sink&lt;T, CompletionStage&lt;Done&gt;&gt; droppySink(
    Sink&lt;T, CompletionStage&lt;Done&gt;&gt; sink, int size) {
  return Flow.&lt;T&gt;create().buffer(size, OverflowStrategy.dropHead()).toMat(sink, Keep.right());
}</code></pre></dd>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeDroppyBroadcast.java#L62-L73" target="_blank" title="Go to snippet source"></a><code class="language-java">RunnableGraph.fromGraph(
    GraphDSL.create(
        builder -&gt; {
          final int outputCount = 3;
          final UniformFanOutShape&lt;Integer, Integer&gt; bcast =
              builder.add(Broadcast.create(outputCount));
          builder.from(builder.add(myData)).toFanOut(bcast);
          builder.from(bcast).to(builder.add(droppySink(mySink1, 10)));
          builder.from(bcast).to(builder.add(droppySink(mySink2, 10)));
          builder.from(bcast).to(builder.add(droppySink(mySink3, 10)));
          return ClosedShape.getInstance();
        }));</code></pre></dd>
</dl>
<h3><a href="#collecting-missed-ticks" name="collecting-missed-ticks" class="anchor"><span class="anchor-link"></span></a>Collecting missed ticks</h3>
<p><strong>Situation:</strong> Given a regular (stream) source of ticks, instead of trying to backpressure the producer of the ticks we want to keep a counter of the missed ticks instead and pass it down when possible.</p>
<p>We will use <code>conflateWithSeed</code> to solve the problem. The seed version of conflate takes two functions:</p>
<ul>
  <li>A seed function that produces the zero element for the folding process that happens when the upstream is faster than the downstream. In our case the seed function is a constant function that returns 0 since there were no missed ticks at that point.</li>
  <li>A fold function that is invoked when multiple upstream messages needs to be collapsed to an aggregate value due to the insufficient processing rate of the downstream. Our folding function increments the currently stored count of the missed ticks so far.</li>
</ul>
<p>As a result, we have a flow of <code>Int</code> where the number represents the missed ticks. A number 0 means that we were able to consume the tick fast enough (i.e. zero means: 1 non-missed tick + 0 missed ticks)</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeMissedTicks.scala#L27-L29" target="_blank" title="Go to snippet source"></a><code class="language-scala">val missedTicks: Flow[Tick, Int, NotUsed] =
  Flow[Tick].conflateWithSeed(seed = (_) ⇒ 0)(
    (missedTicks, tick) ⇒ missedTicks + 1)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeMissedTicks.java#L59-L60" target="_blank" title="Go to snippet source"></a><code class="language-java">final Flow&lt;Tick, Integer, NotUsed&gt; missedTicks =
    Flow.of(Tick.class).conflateWithSeed(tick -&gt; 0, (missed, tick) -&gt; missed + 1);</code></pre></dd>
</dl>
<h3><a href="#create-a-stream-processor-that-repeats-the-last-element-seen" name="create-a-stream-processor-that-repeats-the-last-element-seen" class="anchor"><span class="anchor-link"></span></a>Create a stream processor that repeats the last element seen</h3>
<p><strong>Situation:</strong> Given a producer and consumer, where the rate of neither is known in advance, we want to ensure that none of them is slowing down the other by dropping earlier unconsumed elements from the upstream if necessary, and repeating the last value for the downstream if necessary.</p>
<p>We have two options to implement this feature. In both cases we will use <a href="stream-customize.html"><code>GraphStage</code></a>, to build our custom operator. In the first version we will use a provided initial value <code>initial</code> that will be used to feed the downstream if no upstream element is ready yet. In the <code>onPush()</code> handler we overwrite the <code>currentValue</code> variable and immediately relieve the upstream by calling <code>pull()</code>. The downstream <code>onPull</code> handler is very similar, we immediately relieve the downstream by emitting <code>currentValue</code>.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeHold.scala#L15-L42" target="_blank" title="Go to snippet source"></a><code class="language-scala">import akka.stream._
import akka.stream.stage._
final class HoldWithInitial[T](initial: T) extends GraphStage[FlowShape[T, T]] {
  val in = Inlet[T](&quot;HoldWithInitial.in&quot;)
  val out = Outlet[T](&quot;HoldWithInitial.out&quot;)

  override val shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {
    private var currentValue: T = initial

    setHandlers(in, out, new InHandler with OutHandler {
      override def onPush(): Unit = {
        currentValue = grab(in)
        pull(in)
      }

      override def onPull(): Unit = {
        push(out, currentValue)
      }
    })

    override def preStart(): Unit = {
      pull(in)
    }
  }

}</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeHold.java#L45-L93" target="_blank" title="Go to snippet source"></a><code class="language-java">class HoldWithInitial&lt;T&gt; extends GraphStage&lt;FlowShape&lt;T, T&gt;&gt; {

  public Inlet&lt;T&gt; in = Inlet.&lt;T&gt;create(&quot;HoldWithInitial.in&quot;);
  public Outlet&lt;T&gt; out = Outlet.&lt;T&gt;create(&quot;HoldWithInitial.out&quot;);
  private FlowShape&lt;T, T&gt; shape = FlowShape.of(in, out);

  private final T initial;

  public HoldWithInitial(T initial) {
    this.initial = initial;
  }

  @Override
  public FlowShape&lt;T, T&gt; shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {
      private T currentValue = initial;

      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() throws Exception {
                currentValue = grab(in);
                pull(in);
              }
            });
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                push(out, currentValue);
              }
            });
      }

      @Override
      public void preStart() {
        pull(in);
      }
    };
  }
}</code></pre></dd>
</dl>
<p>While it is relatively simple, the drawback of the first version is that it needs an arbitrary initial element which is not always possible to provide. Hence, we create a second version where the downstream might need to wait in one single case: if the very first element is not yet available.</p>
<p>We introduce a boolean variable <code>waitingFirstValue</code> to denote whether the first element has been provided or not (alternatively an <span class="group-scala"><code>Option</code></span> <span class="group-java"><code>Optional</code></span> can be used for <code>currentValue</code> or if the element type is a subclass of <span class="group-scala"><code>AnyRef</code></span> <span class="group-java"><code>Object</code></span> a null can be used with the same purpose). In the downstream <code>onPull()</code> handler the difference from the previous version is that we check if we have received the first value and only emit if we have. This leads to that when the first element comes in we must check if there possibly already was demand from downstream so that we in that case can push the element directly.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeHold.scala#L46-L77" target="_blank" title="Go to snippet source"></a><code class="language-scala">import akka.stream._
import akka.stream.stage._
final class HoldWithWait[T] extends GraphStage[FlowShape[T, T]] {
  val in = Inlet[T](&quot;HoldWithWait.in&quot;)
  val out = Outlet[T](&quot;HoldWithWait.out&quot;)

  override val shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {
    private var currentValue: T = _
    private var waitingFirstValue = true

    setHandlers(in, out, new InHandler with OutHandler {
      override def onPush(): Unit = {
        currentValue = grab(in)
        if (waitingFirstValue) {
          waitingFirstValue = false
          if (isAvailable(out)) push(out, currentValue)
        }
        pull(in)
      }

      override def onPull(): Unit = {
        if (!waitingFirstValue) push(out, currentValue)
      }
    })

    override def preStart(): Unit = {
      pull(in)
    }
  }
}</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeHold.java#L97-L143" target="_blank" title="Go to snippet source"></a><code class="language-java">class HoldWithWait&lt;T&gt; extends GraphStage&lt;FlowShape&lt;T, T&gt;&gt; {
  public Inlet&lt;T&gt; in = Inlet.&lt;T&gt;create(&quot;HoldWithInitial.in&quot;);
  public Outlet&lt;T&gt; out = Outlet.&lt;T&gt;create(&quot;HoldWithInitial.out&quot;);
  private FlowShape&lt;T, T&gt; shape = FlowShape.of(in, out);

  @Override
  public FlowShape&lt;T, T&gt; shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {
      private T currentValue = null;
      private boolean waitingFirstValue = true;

      {
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() throws Exception {
                currentValue = grab(in);
                if (waitingFirstValue) {
                  waitingFirstValue = false;
                  if (isAvailable(out)) push(out, currentValue);
                }
                pull(in);
              }
            });
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                if (!waitingFirstValue) push(out, currentValue);
              }
            });
      }

      @Override
      public void preStart() {
        pull(in);
      }
    };
  }
}</code></pre></dd>
</dl>
<h3><a href="#globally-limiting-the-rate-of-a-set-of-streams" name="globally-limiting-the-rate-of-a-set-of-streams" class="anchor"><span class="anchor-link"></span></a>Globally limiting the rate of a set of streams</h3>
<p><strong>Situation:</strong> Given a set of independent streams that we cannot merge, we want to globally limit the aggregate throughput of the set of streams.</p>
<p>One possible solution uses a shared actor as the global limiter combined with mapAsync to create a reusable <code>Flow</code> that can be plugged into a stream to limit its rate.</p>
<p>As the first step we define an actor that will do the accounting for the global rate limit. The actor maintains a timer, a counter for pending permit tokens and a queue for possibly waiting participants. The actor has an <code>open</code> and <code>closed</code> state. The actor is in the <code>open</code> state while it has still pending permits. Whenever a request for permit arrives as a <code>WantToPass</code> message to the actor the number of available permits is decremented and we notify the sender that it can pass by answering with a <code>MayPass</code> message. If the amount of permits reaches zero, the actor transitions to the <code>closed</code> state. In this state requests are not immediately answered, instead the reference of the sender is added to a queue. Once the timer for replenishing the pending permits fires by sending a <code>ReplenishTokens</code> message, we increment the pending permits counter and send a reply to each of the waiting senders. If there are more waiting senders than permits available we will stay in the <code>closed</code> state.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeGlobalRateLimit.scala#L22-L80" target="_blank" title="Go to snippet source"></a><code class="language-scala">object Limiter {
  case object WantToPass
  case object MayPass

  case object ReplenishTokens

  def props(maxAvailableTokens: Int, tokenRefreshPeriod: FiniteDuration,
            tokenRefreshAmount: Int): Props =
    Props(new Limiter(maxAvailableTokens, tokenRefreshPeriod, tokenRefreshAmount))
}

class Limiter(
  val maxAvailableTokens: Int,
  val tokenRefreshPeriod: FiniteDuration,
  val tokenRefreshAmount: Int) extends Actor {
  import Limiter._
  import context.dispatcher
  import akka.actor.Status

  private var waitQueue = immutable.Queue.empty[ActorRef]
  private var permitTokens = maxAvailableTokens
  private val replenishTimer = system.scheduler.schedule(
    initialDelay = tokenRefreshPeriod,
    interval = tokenRefreshPeriod,
    receiver = self,
    ReplenishTokens)

  override def receive: Receive = open

  val open: Receive = {
    case ReplenishTokens ⇒
      permitTokens = math.min(permitTokens + tokenRefreshAmount, maxAvailableTokens)
    case WantToPass ⇒
      permitTokens -= 1
      sender() ! MayPass
      if (permitTokens == 0) context.become(closed)
  }

  val closed: Receive = {
    case ReplenishTokens ⇒
      permitTokens = math.min(permitTokens + tokenRefreshAmount, maxAvailableTokens)
      releaseWaiting()
    case WantToPass ⇒
      waitQueue = waitQueue.enqueue(sender())
  }

  private def releaseWaiting(): Unit = {
    val (toBeReleased, remainingQueue) = waitQueue.splitAt(permitTokens)
    waitQueue = remainingQueue
    permitTokens -= toBeReleased.size
    toBeReleased foreach (_ ! MayPass)
    if (permitTokens &gt; 0) context.become(open)
  }

  override def postStop(): Unit = {
    replenishTimer.cancel()
    waitQueue foreach (_ ! Status.Failure(new IllegalStateException(&quot;limiter stopped&quot;)))
  }
}</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeGlobalRateLimit.java#L44-L156" target="_blank" title="Go to snippet source"></a><code class="language-java">static class Limiter extends AbstractActor {

  public static class WantToPass {}

  public static final WantToPass WANT_TO_PASS = new WantToPass();

  public static class MayPass {}

  public static final MayPass MAY_PASS = new MayPass();

  public static class ReplenishTokens {}

  public static final ReplenishTokens REPLENISH_TOKENS = new ReplenishTokens();

  private final int maxAvailableTokens;
  private final Duration tokenRefreshPeriod;
  private final int tokenRefreshAmount;

  private final List&lt;ActorRef&gt; waitQueue = new ArrayList&lt;&gt;();
  private final Cancellable replenishTimer;

  private int permitTokens;

  public static Props props(
      int maxAvailableTokens, Duration tokenRefreshPeriod, int tokenRefreshAmount) {
    return Props.create(
        Limiter.class, maxAvailableTokens, tokenRefreshPeriod, tokenRefreshAmount);
  }

  private Limiter(int maxAvailableTokens, Duration tokenRefreshPeriod, int tokenRefreshAmount) {
    this.maxAvailableTokens = maxAvailableTokens;
    this.tokenRefreshPeriod = tokenRefreshPeriod;
    this.tokenRefreshAmount = tokenRefreshAmount;
    this.permitTokens = maxAvailableTokens;

    this.replenishTimer =
        system
            .scheduler()
            .schedule(
                this.tokenRefreshPeriod,
                this.tokenRefreshPeriod,
                getSelf(),
                REPLENISH_TOKENS,
                getContext().getSystem().dispatcher(),
                getSelf());
  }

  @Override
  public Receive createReceive() {
    return open();
  }

  private Receive open() {
    return receiveBuilder()
        .match(
            ReplenishTokens.class,
            rt -&gt; {
              permitTokens = Math.min(permitTokens + tokenRefreshAmount, maxAvailableTokens);
            })
        .match(
            WantToPass.class,
            wtp -&gt; {
              permitTokens -= 1;
              getSender().tell(MAY_PASS, getSelf());
              if (permitTokens == 0) {
                getContext().become(closed());
              }
            })
        .build();
  }

  private Receive closed() {
    return receiveBuilder()
        .match(
            ReplenishTokens.class,
            rt -&gt; {
              permitTokens = Math.min(permitTokens + tokenRefreshAmount, maxAvailableTokens);
              releaseWaiting();
            })
        .match(
            WantToPass.class,
            wtp -&gt; {
              waitQueue.add(getSender());
            })
        .build();
  }

  private void releaseWaiting() {
    final List&lt;ActorRef&gt; toBeReleased = new ArrayList&lt;&gt;(permitTokens);
    for (Iterator&lt;ActorRef&gt; it = waitQueue.iterator(); permitTokens &gt; 0 &amp;&amp; it.hasNext(); ) {
      toBeReleased.add(it.next());
      it.remove();
      permitTokens--;
    }

    toBeReleased.stream().forEach(ref -&gt; ref.tell(MAY_PASS, getSelf()));
    if (permitTokens &gt; 0) {
      getContext().become(open());
    }
  }

  @Override
  public void postStop() {
    replenishTimer.cancel();
    waitQueue
        .stream()
        .forEach(
            ref -&gt; {
              ref.tell(
                  new Status.Failure(new IllegalStateException(&quot;limiter stopped&quot;)), getSelf());
            });
  }
}</code></pre></dd>
</dl>
<p>To create a Flow that uses this global limiter actor we use the <code>mapAsync</code> function with the combination of the <code>ask</code> pattern. We also define a timeout, so if a reply is not received during the configured maximum wait period the returned future from <code>ask</code> will fail, which will fail the corresponding stream as well.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeGlobalRateLimit.scala#L86-L96" target="_blank" title="Go to snippet source"></a><code class="language-scala">def limitGlobal[T](limiter: ActorRef, maxAllowedWait: FiniteDuration): Flow[T, T, NotUsed] = {
  import akka.pattern.ask
  import akka.util.Timeout
  Flow[T].mapAsync(4)((element: T) ⇒ {
    import system.dispatcher
    implicit val triggerTimeout = Timeout(maxAllowedWait)
    val limiterTriggerFuture = limiter ? Limiter.WantToPass
    limiterTriggerFuture.map((_) ⇒ element)
  })

}</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeGlobalRateLimit.java#L163-L174" target="_blank" title="Go to snippet source"></a><code class="language-java">public &lt;T&gt; Flow&lt;T, T, NotUsed&gt; limitGlobal(ActorRef limiter, Duration maxAllowedWait) {
  final int parallelism = 4;
  final Flow&lt;T, T, NotUsed&gt; f = Flow.create();

  return f.mapAsync(
      parallelism,
      element -&gt; {
        final CompletionStage&lt;Object&gt; limiterTriggerFuture =
            Patterns.ask(limiter, Limiter.WANT_TO_PASS, maxAllowedWait);
        return limiterTriggerFuture.thenApplyAsync(response -&gt; element, system.dispatcher());
      });
}</code></pre></dd>
</dl><div class="callout note "><div class="callout-title">Note</div>
<p>The global actor used for limiting introduces a global bottleneck. You might want to assign a dedicated dispatcher for this actor.</p></div>
<h2><a href="#working-with-io" name="working-with-io" class="anchor"><span class="anchor-link"></span></a>Working with IO</h2>
<h3><a href="#chunking-up-a-stream-of-bytestrings-into-limited-size-bytestrings" name="chunking-up-a-stream-of-bytestrings-into-limited-size-bytestrings" class="anchor"><span class="anchor-link"></span></a>Chunking up a stream of ByteStrings into limited size ByteStrings</h3>
<p><strong>Situation:</strong> Given a stream of <code>ByteString</code> s we want to produce a stream of <code>ByteString</code> s containing the same bytes in the same sequence, but capping the size of <code>ByteString</code> s. In other words we want to slice up <code>ByteString</code> s into smaller chunks if they exceed a size threshold.</p>
<p>This can be achieved with a single <a href="stream-customize.html"><code>GraphStage</code></a> to define a custom Akka Stream operator. The main logic of our operator is in <code>emitChunk()</code> which implements the following logic:</p>
<ul>
  <li>if the buffer is empty, and upstream is not closed we pull for more bytes, if it is closed we complete</li>
  <li>if the buffer is nonEmpty, we split it according to the <code>chunkSize</code>. This will give a next chunk that we will emit, and an empty or nonempty remaining buffer.</li>
</ul>
<p>Both <code>onPush()</code> and <code>onPull()</code> calls <code>emitChunk()</code> the only difference is that the push handler also stores the incoming chunk by appending to the end of the buffer.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeByteStrings.scala#L24-L72" target="_blank" title="Go to snippet source"></a><code class="language-scala">import akka.stream.stage._

class Chunker(val chunkSize: Int) extends GraphStage[FlowShape[ByteString, ByteString]] {
  val in = Inlet[ByteString](&quot;Chunker.in&quot;)
  val out = Outlet[ByteString](&quot;Chunker.out&quot;)
  override val shape = FlowShape.of(in, out)
  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {
    private var buffer = ByteString.empty

    setHandler(out, new OutHandler {
      override def onPull(): Unit = {
        emitChunk()
      }
    })
    setHandler(in, new InHandler {
      override def onPush(): Unit = {
        val elem = grab(in)
        buffer ++= elem
        emitChunk()
      }

      override def onUpstreamFinish(): Unit = {
        if (buffer.isEmpty) completeStage()
        else {
          // There are elements left in buffer, so
          // we keep accepting downstream pulls and push from buffer until emptied.
          //
          // It might be though, that the upstream finished while it was pulled, in which
          // case we will not get an onPull from the downstream, because we already had one.
          // In that case we need to emit from the buffer.
          if (isAvailable(out)) emitChunk()
        }
      }
    })

    private def emitChunk(): Unit = {
      if (buffer.isEmpty) {
        if (isClosed(in)) completeStage()
        else pull(in)
      } else {
        val (chunk, nextBuffer) = buffer.splitAt(chunkSize)
        buffer = nextBuffer
        push(out, chunk)
      }
    }
  }
}

val chunksStream = rawBytes.via(new Chunker(ChunkLimit))</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeByteStrings.java#L61-L135" target="_blank" title="Go to snippet source"></a><code class="language-java">class Chunker extends GraphStage&lt;FlowShape&lt;ByteString, ByteString&gt;&gt; {

  private final int chunkSize;

  public Inlet&lt;ByteString&gt; in = Inlet.&lt;ByteString&gt;create(&quot;Chunker.in&quot;);
  public Outlet&lt;ByteString&gt; out = Outlet.&lt;ByteString&gt;create(&quot;Chunker.out&quot;);
  private FlowShape&lt;ByteString, ByteString&gt; shape = FlowShape.of(in, out);

  public Chunker(int chunkSize) {
    this.chunkSize = chunkSize;
  }

  @Override
  public FlowShape&lt;ByteString, ByteString&gt; shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {
      private ByteString buffer = ByteString.empty();

      {
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                emitChunk();
              }
            });

        setHandler(
            in,
            new AbstractInHandler() {

              @Override
              public void onPush() throws Exception {
                ByteString elem = grab(in);
                buffer = buffer.concat(elem);
                emitChunk();
              }

              @Override
              public void onUpstreamFinish() throws Exception {
                if (buffer.isEmpty()) completeStage();
                else {
                  // There are elements left in buffer, so
                  // we keep accepting downstream pulls and push from buffer until emptied.
                  //
                  // It might be though, that the upstream finished while it was pulled, in
                  // which
                  // case we will not get an onPull from the downstream, because we already
                  // had one.
                  // In that case we need to emit from the buffer.
                  if (isAvailable(out)) emitChunk();
                }
              }
            });
      }

      private void emitChunk() {
        if (buffer.isEmpty()) {
          if (isClosed(in)) completeStage();
          else pull(in);
        } else {
          Tuple2&lt;ByteString, ByteString&gt; split = buffer.splitAt(chunkSize);
          ByteString chunk = split._1();
          buffer = split._2();
          push(out, chunk);
        }
      }
    };
  }
}</code></pre></dd>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeByteStrings.java#L140" target="_blank" title="Go to snippet source"></a><code class="language-java">Source&lt;ByteString, NotUsed&gt; chunksStream = rawBytes.via(new Chunker(CHUNK_LIMIT));</code></pre></dd>
</dl>
<h3><a href="#limit-the-number-of-bytes-passing-through-a-stream-of-bytestrings" name="limit-the-number-of-bytes-passing-through-a-stream-of-bytestrings" class="anchor"><span class="anchor-link"></span></a>Limit the number of bytes passing through a stream of ByteStrings</h3>
<p><strong>Situation:</strong> Given a stream of <code>ByteString</code> s we want to fail the stream if more than a given maximum of bytes has been consumed.</p>
<p>This recipe uses a <a href="stream-customize.html"><code>GraphStage</code></a> to implement the desired feature. In the only handler we override, <code>onPush()</code> we update a counter and see if it gets larger than <code>maximumBytes</code>. If a violation happens we signal failure, otherwise we forward the chunk we have received.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeByteStrings.scala#L86-L111" target="_blank" title="Go to snippet source"></a><code class="language-scala">import akka.stream.stage._
class ByteLimiter(val maximumBytes: Long) extends GraphStage[FlowShape[ByteString, ByteString]] {
  val in = Inlet[ByteString](&quot;ByteLimiter.in&quot;)
  val out = Outlet[ByteString](&quot;ByteLimiter.out&quot;)
  override val shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes): GraphStageLogic = new GraphStageLogic(shape) {
    private var count = 0

    setHandlers(in, out, new InHandler with OutHandler {

      override def onPull(): Unit = {
        pull(in)
      }

      override def onPush(): Unit = {
        val chunk = grab(in)
        count += chunk.size
        if (count &gt; maximumBytes) failStage(new IllegalStateException(&quot;Too much bytes&quot;))
        else push(out, chunk)
      }
    })
  }
}

val limiter = Flow[ByteString].via(new ByteLimiter(SizeLimit))</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeByteStrings.java#L167-L215" target="_blank" title="Go to snippet source"></a><code class="language-java">class ByteLimiter extends GraphStage&lt;FlowShape&lt;ByteString, ByteString&gt;&gt; {

  final long maximumBytes;

  public Inlet&lt;ByteString&gt; in = Inlet.&lt;ByteString&gt;create(&quot;ByteLimiter.in&quot;);
  public Outlet&lt;ByteString&gt; out = Outlet.&lt;ByteString&gt;create(&quot;ByteLimiter.out&quot;);
  private FlowShape&lt;ByteString, ByteString&gt; shape = FlowShape.of(in, out);

  public ByteLimiter(long maximumBytes) {
    this.maximumBytes = maximumBytes;
  }

  @Override
  public FlowShape&lt;ByteString, ByteString&gt; shape() {
    return shape;
  }

  @Override
  public GraphStageLogic createLogic(Attributes inheritedAttributes) {
    return new GraphStageLogic(shape) {
      private int count = 0;

      {
        setHandler(
            out,
            new AbstractOutHandler() {
              @Override
              public void onPull() throws Exception {
                pull(in);
              }
            });
        setHandler(
            in,
            new AbstractInHandler() {
              @Override
              public void onPush() throws Exception {
                ByteString chunk = grab(in);
                count += chunk.size();
                if (count &gt; maximumBytes) {
                  failStage(new IllegalStateException(&quot;Too much bytes&quot;));
                } else {
                  push(out, chunk);
                }
              }
            });
      }
    };
  }
}</code></pre>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeByteStrings.java#L220-L221" target="_blank" title="Go to snippet source"></a><code class="language-java">Flow&lt;ByteString, ByteString, NotUsed&gt; limiter =
    Flow.of(ByteString.class).via(new ByteLimiter(SIZE_LIMIT));</code></pre></dd>
</dl>
<h3><a href="#compact-bytestrings-in-a-stream-of-bytestrings" name="compact-bytestrings-in-a-stream-of-bytestrings" class="anchor"><span class="anchor-link"></span></a>Compact ByteStrings in a stream of ByteStrings</h3>
<p><strong>Situation:</strong> After a long stream of transformations, due to their immutable, structural sharing nature <code>ByteString</code> s may refer to multiple original ByteString instances unnecessarily retaining memory. As the final step of a transformation chain we want to have clean copies that are no longer referencing the original <code>ByteString</code> s.</p>
<p>The recipe is a simple use of map, calling the <code>compact()</code> method of the <code>ByteString</code> elements. This does copying of the underlying arrays, so this should be the last element of a long chain if used.</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeByteStrings.scala#L130" target="_blank" title="Go to snippet source"></a><code class="language-scala">val compacted: Source[ByteString, NotUsed] = data.map(_.compact)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeByteStrings.java#L283" target="_blank" title="Go to snippet source"></a><code class="language-java">Source&lt;ByteString, NotUsed&gt; compacted = rawBytes.map(ByteString::compact);</code></pre></dd>
</dl>
<h3><a href="#injecting-keep-alive-messages-into-a-stream-of-bytestrings" name="injecting-keep-alive-messages-into-a-stream-of-bytestrings" class="anchor"><span class="anchor-link"></span></a>Injecting keep-alive messages into a stream of ByteStrings</h3>
<p><strong>Situation:</strong> Given a communication channel expressed as a stream of <code>ByteString</code> s we want to inject keep-alive messages but only if this does not interfere with normal traffic.</p>
<p>There is a built-in operation that allows to do this directly:</p>
<dl>
  <dt>Scala</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/scala/docs/stream/cookbook/RecipeKeepAlive.scala#L19-L21" target="_blank" title="Go to snippet source"></a><code class="language-scala">import scala.concurrent.duration._
val injectKeepAlive: Flow[ByteString, ByteString, NotUsed] =
  Flow[ByteString].keepAlive(1.second, () ⇒ keepaliveMessage)</code></pre></dd>
  <dt>Java</dt>
  <dd>
  <pre class="prettyprint"><a class="icon go-to-source" href="https://github.com/xmeng1/akka/tree/master/akka-docs/src/test/java/jdocs/stream/javadsl/cookbook/RecipeKeepAlive.java#L49-L50" target="_blank" title="Go to snippet source"></a><code class="language-java">Flow&lt;ByteString, ByteString, NotUsed&gt; keepAliveInject =
    Flow.of(ByteString.class).keepAlive(Duration.ofSeconds(1), () -&gt; keepAliveMessage);</code></pre></dd>
</dl>
</div>
</article>

<div class="row">
<div class="small-12 large-9 column">
<section class="nav-prev-next row">
<div class="nav-prev small-6 column">
<a href="../stream/stream-substream.html"><i class="icon-prev"></i> <span class="link-prev">Substreams</span></a>
</div>
<div class="nav-next small-6 column clearfix">
<a class="float-right" href="../general/stream/stream-configuration.html">Configuration <i class="icon-next"></i></a>
</div>
</section>
</div>
</div>

<div class="source-github row">
Found an error in this documentation? The source code for this page can be found <a href="https://github.com/xmeng1/akka/tree/master/akka-docs-cn/src/main/paradox/stream/stream-cookbook.md">here</a>.
Please feel free to edit and contribute a pull request.
</div>


<footer class="page-footer row clearfix">
<img class="akka-icon float-left show-for-medium" src="../images/akka-icon.svg">
<section class="copyright">
<div>Akka is Open Source and available under the Apache 2 License.</div>
<p class="legal">
&copy; 2011-2019 <a href="https://www.lightbend.com" target="_blank">Lightbend, Inc.</a> | 
<a href="https://www.lightbend.com/legal/licenses" target="_blank">Licenses</a> | 
<a href="https://www.lightbend.com/legal/terms" target="_blank">Terms</a> | 
<a href="https://www.lightbend.com/legal/privacy" target="_blank">Privacy Policy</a> | 
<a href="https://akka.io/cookie/" target="_blank">Cookie Listing</a> | 
<a class="optanon-toggle-display">Cookie Settings</a>
</p>
</section>
</footer>

</section>
</main>

<script type="text/javascript" src="../js/scrollsneak.js"></script>
<script type="text/javascript">jQuery(document).foundation();</script>
<script type="text/javascript" src="../js/groups.js"></script>
<script type="text/javascript" src="../js/page.js"></script>
<script type="text/javascript" src="../js/magellan.js"></script>

<style type="text/css">@import "../lib/prettify/prettify.css";</style>
<script type="text/javascript" src="../lib/prettify/prettify.js"></script>
<script type="text/javascript" src="../lib/prettify/lang-scala.js"></script>
<script type="text/javascript">jQuery(function(){window.prettyPrint && prettyPrint()});</script>

<!-- Algolia docs search -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/2/docsearch.min.js"></script>
<style>.algolia-autocomplete { display: block !important }</style>
<script type="text/javascript">
docsearch({
apiKey: '543bad5ad786495d9ccd445ed34ed082',
indexName: 'akka_io',
inputSelector: '#search',
algoliaOptions: {
hitsPerPage: 5
}
});

docsearch({
apiKey: '543bad5ad786495d9ccd445ed34ed082',
indexName: 'akka_io',
inputSelector: '#overlay-search',
algoliaOptions: {
hitsPerPage: 5
}
});

// set up "/" as global shortcut for focusing on search
jQuery(document).keypress(function (event) {
if (event.keyCode == 47) {
jQuery("#search").focus();
return false; // swallow key event, otherwise the / char would be input into the search box
}
});
</script>

<script type="text/javascript" src="../assets/js/warnOldDocs.js"></script>
<script type="text/javascript" src="../assets/js/scalafiddle.js"></script>


</body>
</html>
